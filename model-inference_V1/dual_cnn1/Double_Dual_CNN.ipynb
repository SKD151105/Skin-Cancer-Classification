{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f70b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768eabc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d1c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/mini_proj_data/processed_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b45b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71adcadf",
   "metadata": {},
   "source": [
    "# 1. With Processed Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_path']   # paths to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ec116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess_image(path, target_size=(224,224)):\n",
    "    img = cv2.imread(path)                      # read image\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  \n",
    "    img = cv2.resize(img, target_size)\n",
    "    img = img / 255.0                           # normalize to [0,1]\n",
    "    return img.astype('float32')\n",
    "\n",
    "\n",
    "X_images=np.array([load_and_preprocess_image(p) for p in X])\n",
    "\n",
    "# --- Standardization step (per-channel) ---\n",
    "mean = np.mean(X_images, axis=(0,1,2), keepdims=True)\n",
    "std  = np.std(X_images, axis=(0,1,2), keepdims=True)\n",
    "\n",
    "X_images = (X_images - mean) / (std + 1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c3f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a39a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, regularizers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dropout, Input\n",
    "import tensorflow as tf\n",
    "\n",
    "# ---------------- CNN1 ----------------\n",
    "def build_cnn1(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(32, (3,3), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    x = Conv2D(128, (3,3), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    x = Conv2D(256, (3,3), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)   # still useful to avoid overfitting\n",
    "    return Model(inputs, x, name=\"CNN1\")\n",
    "\n",
    "# ---------------- CNN2 ----------------\n",
    "def build_cnn2(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(256, (7,7), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "    x = AveragePooling2D((2,2))(x)\n",
    "\n",
    "    x = Conv2D(128, (5,5), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = AveragePooling2D((2,2))(x)\n",
    "\n",
    "    x = Conv2D(96, (3,3), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = AveragePooling2D((2,2))(x)\n",
    "\n",
    "    x = Conv2D(96, (3,3), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = AveragePooling2D((2,2))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    return Model(inputs, x, name=\"CNN2\")\n",
    "\n",
    "# ---------------- CNN3 ----------------\n",
    "def build_cnn3(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(32, (3,3), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    x = Conv2D(96, (5,5), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    x = Conv2D(128, (5, 5), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    x = Conv2D(256, (7, 7), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)   # still useful to avoid overfitting\n",
    "    return Model(inputs, x, name=\"CNN1\")\n",
    "\n",
    "\n",
    "# ---------------- CNN4 ----------------\n",
    "def build_cnn4(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(32, (3,3), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "    x = AveragePooling2D((3,3))(x)\n",
    "\n",
    "    x = Conv2D(32, (3,3), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = AveragePooling2D((5,5))(x)\n",
    "\n",
    "    x = Conv2D(64, (5,5), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = AveragePooling2D((3,3))(x)\n",
    "\n",
    "    x = Conv2D(128, (5,5), activation='relu', padding=\"same\",\n",
    "               kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = AveragePooling2D((3,3))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    return Model(inputs, x, name=\"CNN2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc716d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "# datagen.fit(X_train_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2994e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "CNN1 = build_cnn1(input_shape)\n",
    "CNN2 = build_cnn2(input_shape)\n",
    "CNN3 = build_cnn3(input_shape)\n",
    "CNN4 = build_cnn4(input_shape)\n",
    "print(\"---- CNN1 Summary ----\")\n",
    "CNN1.summary()\n",
    "\n",
    "print(\"\\n---- CNN2 Summary ----\")\n",
    "CNN2.summary()\n",
    "\n",
    "print(\"\\n---- CNN3 Summary ----\")\n",
    "CNN3.summary()\n",
    "\n",
    "print(\"\\n---- CNN4 Summary ----\")\n",
    "CNN4.summary()\n",
    "# Use augmented data for feature extraction\n",
    "# train_generator = datagen.flow(X_train_images, y_train, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dbe7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cnn1 = CNN1.predict(X_images, verbose=1)\n",
    "features_cnn2 = CNN2.predict(X_images, verbose=1)\n",
    "features_cnn3 = CNN3.predict(X_images, verbose=1)\n",
    "features_cnn4 = CNN4.predict(X_images, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "all_features = np.concatenate([features_cnn1, features_cnn2], axis=1)\n",
    "features_pca = pca.fit_transform(all_features)\n",
    "\n",
    "# later you can split back if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f10698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "# Concatenate features from CNN3 and CNN4\n",
    "all_features_dual2 = np.concatenate([features_cnn3, features_cnn4], axis=1)\n",
    "\n",
    "# Apply Factor Analysis to reduce dimensionality\n",
    "fa = FactorAnalysis(n_components=50, random_state=42)\n",
    "features_fa = fa.fit_transform(all_features_dual2)\n",
    "\n",
    "# features_fa now contains the reduced features for dual2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20ca661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Merge features from both Dual CNNs\n",
    "# features_pca: reduced features from Dual-1 (PCA)\n",
    "# features_fa: reduced features from Dual-2 (FA)\n",
    "merged_features = np.concatenate([features_pca, features_fa], axis=1)\n",
    "\n",
    "# Step 2: Remove duplicate features (columns)\n",
    "# Convert to DataFrame for easy duplicate removal\n",
    "import pandas as pd\n",
    "merged_df = pd.DataFrame(merged_features)\n",
    "# Remove duplicate columns\n",
    "merged_df = merged_df.loc[:, ~merged_df.T.duplicated()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Train + temp (val+test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    merged_df.values, df['label'].values, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Validation + Test (split from temp)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=1/3, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:  \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "# ---------------- Classifier ----------------\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "# Early stopping callback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f9a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,   # <-- use your numpy labels directly\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=80,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop], \n",
    "       # <- stops when val_acc stops improving\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a117104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "def metrics():\n",
    "    # Evaluate model on test data\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(f\"âœ… Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"âœ… Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    y_pred_probs = model.predict(X_test)             # probabilities (N, 7)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)  \n",
    "    \n",
    "\n",
    "    print(\"\\nðŸ“Š Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4)) \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[f\"Class {i}\" for i in range(7)],\n",
    "                yticklabels=[f\"Class {i}\" for i in range(7)])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    # Binarize test labels (one-hot for ROC)\n",
    "    y_test_bin = label_binarize(y_test, classes=np.arange(7))\n",
    "\n",
    "    # ROC-AUC (macro average)\n",
    "    roc_auc = roc_auc_score(y_test_bin, y_pred_probs, multi_class=\"ovr\")\n",
    "    print(f\" ROC-AUC (macro): {roc_auc:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1b6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name=\"dual_cnn_classifier\"):\n",
    "    model_dir = \"/content/drive/MyDrive/mini_proj_data/saved_models/\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_path = os.path.join(model_dir, model_name + \".h5\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f659feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bf53f75",
   "metadata": {},
   "source": [
    "# 2. Processed images with data splitting outside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Split paths and labels first\n",
    "paths = df['clean_path'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# 70% train, 30% temp\n",
    "paths_train, paths_temp, labels_train, labels_temp = train_test_split(\n",
    "    paths, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "# 20% test, 10% val from temp\n",
    "paths_test, paths_val, labels_test, labels_val = train_test_split(\n",
    "    paths_temp, labels_temp, test_size=1/3, random_state=42, stratify=labels_temp\n",
    ")\n",
    "\n",
    "# Step 2: Preprocess images for each split\n",
    "X_train_images = np.array([load_and_preprocess_image(p) for p in paths_train])\n",
    "X_test_images = np.array([load_and_preprocess_image(p) for p in paths_test])\n",
    "X_val_images = np.array([load_and_preprocess_image(p) for p in paths_val])\n",
    "\n",
    "# Step 3: Feature extraction for each split\n",
    "features_cnn1_train = CNN1.predict(X_train_images, verbose=1)\n",
    "features_cnn2_train = CNN2.predict(X_train_images, verbose=1)\n",
    "# ... repeat for test and val sets\n",
    "\n",
    "# Step 4: Dimensionality reduction, merging, deduplication for each split\n",
    "# (Apply PCA/FA on train, transform test/val using fitted models)\n",
    "\n",
    "# Step 5: Train classifier on train set, evaluate on test and val sets\n",
    "# (Use metrics like accuracy, confusion matrix, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3242e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Dimensionality reduction, merging, deduplication for each split\n",
    "\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "\n",
    "# Fit PCA and FA on train set only\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "fa = FactorAnalysis(n_components=50, random_state=42)\n",
    "\n",
    "# Extract features for all CNNs and splits\n",
    "features_cnn1_train = CNN1.predict(X_train_images, verbose=1)\n",
    "features_cnn2_train = CNN2.predict(X_train_images, verbose=1)\n",
    "features_cnn3_train = CNN3.predict(X_train_images, verbose=1)\n",
    "features_cnn4_train = CNN4.predict(X_train_images, verbose=1)\n",
    "\n",
    "features_cnn1_test = CNN1.predict(X_test_images, verbose=1)\n",
    "features_cnn2_test = CNN2.predict(X_test_images, verbose=1)\n",
    "features_cnn3_test = CNN3.predict(X_test_images, verbose=1)\n",
    "features_cnn4_test = CNN4.predict(X_test_images, verbose=1)\n",
    "\n",
    "features_cnn1_val = CNN1.predict(X_val_images, verbose=1)\n",
    "features_cnn2_val = CNN2.predict(X_val_images, verbose=1)\n",
    "features_cnn3_val = CNN3.predict(X_val_images, verbose=1)\n",
    "features_cnn4_val = CNN4.predict(X_val_images, verbose=1)\n",
    "\n",
    "# Dual-1: PCA on CNN1+CNN2\n",
    "all_features_train_dual1 = np.concatenate([features_cnn1_train, features_cnn2_train], axis=1)\n",
    "features_pca_train = pca.fit_transform(all_features_train_dual1)\n",
    "\n",
    "all_features_test_dual1 = np.concatenate([features_cnn1_test, features_cnn2_test], axis=1)\n",
    "features_pca_test = pca.transform(all_features_test_dual1)\n",
    "\n",
    "all_features_val_dual1 = np.concatenate([features_cnn1_val, features_cnn2_val], axis=1)\n",
    "features_pca_val = pca.transform(all_features_val_dual1)\n",
    "\n",
    "# Dual-2: FA on CNN3+CNN4\n",
    "all_features_train_dual2 = np.concatenate([features_cnn3_train, features_cnn4_train], axis=1)\n",
    "features_fa_train = fa.fit_transform(all_features_train_dual2)\n",
    "\n",
    "all_features_test_dual2 = np.concatenate([features_cnn3_test, features_cnn4_test], axis=1)\n",
    "features_fa_test = fa.transform(all_features_test_dual2)\n",
    "\n",
    "all_features_val_dual2 = np.concatenate([features_cnn3_val, features_cnn4_val], axis=1)\n",
    "features_fa_val = fa.transform(all_features_val_dual2)\n",
    "\n",
    "# Merge features\n",
    "merged_train = np.concatenate([features_pca_train, features_fa_train], axis=1)\n",
    "merged_test = np.concatenate([features_pca_test, features_fa_test], axis=1)\n",
    "merged_val = np.concatenate([features_pca_val, features_fa_val], axis=1)\n",
    "\n",
    "# Remove duplicate columns\n",
    "import pandas as pd\n",
    "merged_train_df = pd.DataFrame(merged_train)\n",
    "merged_test_df = pd.DataFrame(merged_test)\n",
    "merged_val_df = pd.DataFrame(merged_val)\n",
    "\n",
    "# Use train columns to remove duplicates\n",
    "unique_cols = ~merged_train_df.T.duplicated()\n",
    "merged_train_df = merged_train_df.loc[:, unique_cols]\n",
    "merged_test_df = merged_test_df.loc[:, unique_cols]\n",
    "merged_val_df = merged_val_df.loc[:, unique_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e425913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train classifier and evaluate metrics\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(merged_train_df.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    merged_train_df.values, labels_train,\n",
    "    validation_data=(merged_val_df.values, labels_val),\n",
    "    epochs=80,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(merged_test_df.values, labels_test, verbose=1)\n",
    "print(f\"âœ… Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"âœ… Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred_probs = model.predict(merged_test_df.values)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(labels_test, y_pred, digits=4))\n",
    "cm = confusion_matrix(labels_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[f\"Class {i}\" for i in range(7)],\n",
    "            yticklabels=[f\"Class {i}\" for i in range(7)])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c4b9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5e113d8",
   "metadata": {},
   "source": [
    "# 2. Un processed images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d7826",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48172dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c45f577",
   "metadata": {},
   "source": [
    "# 3. Unprocessed images with train-test outside arch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d84156",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561204a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
