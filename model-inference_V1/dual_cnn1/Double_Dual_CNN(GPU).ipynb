{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e406c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 0. Mount Drive & Imports\n",
    "# =============================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5208e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, cv2, tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Directories\n",
    "base_dir = \"/content/drive/MyDrive/mini_proj_data/\"\n",
    "cache_dir = os.path.join(base_dir, \"cache/\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Try RAPIDS cuML for GPU PCA/FA\n",
    "use_cuml = False\n",
    "try:\n",
    "    from cuml.decomposition import PCA as cuPCA\n",
    "    from cuml.decomposition import FactorAnalysis as cuFA\n",
    "    import cupy as cp\n",
    "    use_cuml = True\n",
    "    print(\"‚úÖ cuML detected ‚Üí GPU accelerated PCA & FactorAnalysis will be used.\")\n",
    "except ImportError:\n",
    "    from sklearn.decomposition import PCA, FactorAnalysis\n",
    "    print(\"‚ö†Ô∏è cuML not available ‚Üí Falling back to sklearn (CPU).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 1. Load Data\n",
    "# =============================\n",
    "df = pd.read_csv(os.path.join(base_dir, \"processed_data.csv\"))\n",
    "print(\"Data loaded:\", df.shape)\n",
    "\n",
    "X = df['clean_path']\n",
    "y = df['label'].values\n",
    "\n",
    "# =============================\n",
    "# 2. Preprocess Images (with caching)\n",
    "# =============================\n",
    "img_cache = os.path.join(cache_dir, \"X_images.npy\")\n",
    "\n",
    "if os.path.exists(img_cache):\n",
    "    print(\"üîÑ Loading preprocessed images from cache...\")\n",
    "    X_images = np.load(img_cache)\n",
    "else:\n",
    "    print(\"‚ö° Preprocessing images...\")\n",
    "    def load_and_preprocess_image(path, target_size=(224,224)):\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, target_size)\n",
    "        img = img / 255.0\n",
    "        return img.astype('float32')\n",
    "\n",
    "    X_images = np.array([load_and_preprocess_image(p) for p in X])\n",
    "    # Standardization\n",
    "    mean = np.mean(X_images, axis=(0,1,2), keepdims=True)\n",
    "    std  = np.std(X_images, axis=(0,1,2), keepdims=True)\n",
    "    X_images = (X_images - mean) / (std + 1e-7)\n",
    "    np.save(img_cache, X_images)\n",
    "\n",
    "print(\"‚úÖ Images ready:\", X_images.shape)\n",
    "\n",
    "# =============================\n",
    "# 3. Define CNN Feature Extractors\n",
    "# =============================\n",
    "from tensorflow.keras import Model, regularizers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dropout, Input\n",
    "\n",
    "def build_cnn1(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(64,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(128,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(256,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Flatten()(x); x = Dropout(0.5)(x)\n",
    "    return Model(inputs, x, name=\"CNN1\")\n",
    "\n",
    "def build_cnn2(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(256,(7,7),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "    x = AveragePooling2D((2,2))(x)\n",
    "    x = Conv2D(128,(5,5),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = AveragePooling2D((2,2))(x)\n",
    "    x = Conv2D(96,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = AveragePooling2D((2,2))(x)\n",
    "    x = Conv2D(96,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = AveragePooling2D((2,2))(x)\n",
    "    x = Flatten()(x); x = Dropout(0.5)(x)\n",
    "    return Model(inputs, x, name=\"CNN2\")\n",
    "\n",
    "def build_cnn3(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(96,(5,5),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(128,(5,5),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(256,(7,7),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Flatten()(x); x = Dropout(0.5)(x)\n",
    "    return Model(inputs, x, name=\"CNN3\")\n",
    "\n",
    "def build_cnn4(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "    x = AveragePooling2D((3,3))(x)\n",
    "    x = Conv2D(32,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = AveragePooling2D((5,5))(x)\n",
    "    x = Conv2D(64,(5,5),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = AveragePooling2D((3,3))(x)\n",
    "    x = Conv2D(128,(5,5),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = AveragePooling2D((3,3))(x)\n",
    "    x = Flatten()(x); x = Dropout(0.5)(x)\n",
    "    return Model(inputs, x, name=\"CNN4\")\n",
    "\n",
    "input_shape = (224,224,3)\n",
    "CNN1, CNN2, CNN3, CNN4 = build_cnn1(input_shape), build_cnn2(input_shape), build_cnn3(input_shape), build_cnn4(input_shape)\n",
    "\n",
    "# =============================\n",
    "# 4. Feature Extraction (with caching)\n",
    "# =============================\n",
    "def get_or_compute_features(model, X_images, name):\n",
    "    cache_file = os.path.join(cache_dir, f\"{name}.npy\")\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"üîÑ Loading cached {name} features...\")\n",
    "        return np.load(cache_file)\n",
    "    print(f\"‚ö° Extracting {name} features...\")\n",
    "    with tf.device('/GPU:0'):\n",
    "        feats = model.predict(X_images, batch_size=128, verbose=1)\n",
    "    np.save(cache_file, feats)\n",
    "    return feats\n",
    "\n",
    "features_cnn1 = get_or_compute_features(CNN1, X_images, \"features_cnn1\")\n",
    "features_cnn2 = get_or_compute_features(CNN2, X_images, \"features_cnn2\")\n",
    "features_cnn3 = get_or_compute_features(CNN3, X_images, \"features_cnn3\")\n",
    "features_cnn4 = get_or_compute_features(CNN4, X_images, \"features_cnn4\")\n",
    "\n",
    "# =============================\n",
    "# 5. Dimensionality Reduction (with caching)\n",
    "# =============================\n",
    "merged_cache = os.path.join(cache_dir, \"merged_features.npy\")\n",
    "\n",
    "if os.path.exists(merged_cache):\n",
    "    print(\"üîÑ Loading merged PCA+FA features from cache...\")\n",
    "    merged_features = np.load(merged_cache)\n",
    "else:\n",
    "    print(\"Applying PCA on CNN1+CNN2...\")\n",
    "    all_features_dual1 = np.concatenate([features_cnn1, features_cnn2], axis=1)\n",
    "    if use_cuml:\n",
    "        pca = cuPCA(n_components=50)\n",
    "        features_pca = pca.fit_transform(cp.asarray(all_features_dual1))\n",
    "        features_pca = cp.asnumpy(features_pca)\n",
    "    else:\n",
    "        pca = PCA(n_components=50)\n",
    "        features_pca = pca.fit_transform(all_features_dual1)\n",
    "\n",
    "    print(\"Applying FA on CNN3+CNN4...\")\n",
    "    all_features_dual2 = np.concatenate([features_cnn3, features_cnn4], axis=1)\n",
    "    if use_cuml:\n",
    "        fa = cuFA(n_components=50, random_state=42)\n",
    "        features_fa = fa.fit_transform(cp.asarray(all_features_dual2))\n",
    "        features_fa = cp.asnumpy(features_fa)\n",
    "    else:\n",
    "        fa = FactorAnalysis(n_components=50, random_state=42)\n",
    "        features_fa = fa.fit_transform(all_features_dual2)\n",
    "\n",
    "    merged_features = np.concatenate([features_pca, features_fa], axis=1)\n",
    "    np.save(merged_cache, merged_features)\n",
    "\n",
    "print(\"‚úÖ Features ready:\", merged_features.shape)\n",
    "\n",
    "# =============================\n",
    "# 6. Train/Val/Test Split\n",
    "# =============================\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    merged_features, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=1/3, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:  \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)\n",
    "\n",
    "# =============================\n",
    "# 7. Classifier Training\n",
    "# =============================\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model_path = os.path.join(base_dir, \"saved_models/dual_cnn_classifier.h5\")\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"üîÑ Loading saved classifier model...\")\n",
    "    from tensorflow.keras.models import load_model\n",
    "    model = load_model(model_path)\n",
    "else:\n",
    "    print(\"‚ö° Training new classifier...\")\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dropout(0.2),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(7, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=80,\n",
    "        batch_size=128,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "    model.save(model_path)\n",
    "    print(f\"üíæ Model saved at {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
