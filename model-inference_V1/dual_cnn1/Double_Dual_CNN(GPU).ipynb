{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =============================\n# 0. Mount Drive & Imports\n# =============================\nfrom google.colab import drive\ndrive.mount('/content/drive')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T07:39:35.860336Z","iopub.execute_input":"2025-09-14T07:39:35.860662Z","iopub.status.idle":"2025-09-14T07:39:35.986721Z","shell.execute_reply.started":"2025-09-14T07:39:35.860624Z","shell.execute_reply":"2025-09-14T07:39:35.985401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport os, cv2, tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Directories\nbase_dir = \"/content/drive/MyDrive/mini_proj_data/\"\ncache_dir = os.path.join(base_dir, \"cache/\")\nos.makedirs(cache_dir, exist_ok=True)\n\n# Try RAPIDS cuML for GPU PCA/FA\nuse_cuml = False\ntry:\n    from cuml.decomposition import PCA as cuPCA\n    from cuml.decomposition import FactorAnalysis as cuFA\n    import cupy as cp\n    use_cuml = True\n    print(\"‚úÖ cuML detected ‚Üí GPU accelerated PCA & FactorAnalysis will be used.\")\nexcept ImportError:\n    from sklearn.decomposition import PCA, FactorAnalysis\n    print(\"‚ö†Ô∏è cuML not available ‚Üí Falling back to sklearn (CPU).\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# =============================\n# 1. Load Data\n# =============================\ndf = pd.read_csv(os.path.join(base_dir, \"processed_data.csv\"))\nprint(\"Data loaded:\", df.shape)\n\nX = df['clean_path']\ny = df['label'].values\n\n# =============================\n# 2. Preprocess Images (with caching)\n# =============================\nimg_cache = os.path.join(cache_dir, \"X_images.npy\")\n\nif os.path.exists(img_cache):\n    print(\"üîÑ Loading preprocessed images from cache...\")\n    X_images = np.load(img_cache)\nelse:\n    print(\"‚ö° Preprocessing images...\")\n    def load_and_preprocess_image(path, target_size=(224,224)):\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, target_size)\n        img = img / 255.0\n        return img.astype('float32')\n\n    X_images = np.array([load_and_preprocess_image(p) for p in X])\n    # Standardization\n    mean = np.mean(X_images, axis=(0,1,2), keepdims=True)\n    std  = np.std(X_images, axis=(0,1,2), keepdims=True)\n    X_images = (X_images - mean) / (std + 1e-7)\n    np.save(img_cache, X_images)\n\nprint(\"‚úÖ Images ready:\", X_images.shape)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# =============================\n# 3. Define CNN Feature Extractors\n# =============================\nfrom tensorflow.keras import Model, regularizers\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dropout, Input\n\ndef build_cnn1(input_shape):\n    inputs = Input(shape=input_shape)\n    x = Conv2D(32,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(inputs)\n    x = MaxPooling2D((2,2))(x)\n    x = Conv2D(64,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = MaxPooling2D((2,2))(x)\n    x = Conv2D(128,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = MaxPooling2D((2,2))(x)\n    x = Conv2D(256,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = MaxPooling2D((2,2))(x)\n    x = Flatten()(x); x = Dropout(0.5)(x)\n    return Model(inputs, x, name=\"CNN1\")\n\ndef build_cnn2(input_shape):\n    inputs = Input(shape=input_shape)\n    x = Conv2D(256,(7,7),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(inputs)\n    x = AveragePooling2D((2,2))(x)\n    x = Conv2D(128,(5,5),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = AveragePooling2D((2,2))(x)\n    x = Conv2D(96,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = AveragePooling2D((2,2))(x)\n    x = Conv2D(96,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = AveragePooling2D((2,2))(x)\n    x = Flatten()(x); x = Dropout(0.5)(x)\n    return Model(inputs, x, name=\"CNN2\")\n\ndef build_cnn3(input_shape):\n    inputs = Input(shape=input_shape)\n    x = Conv2D(32,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(inputs)\n    x = MaxPooling2D((2,2))(x)\n    x = Conv2D(96,(5,5),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = MaxPooling2D((2,2))(x)\n    x = Conv2D(128,(5,5),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = MaxPooling2D((2,2))(x)\n    x = Conv2D(256,(7,7),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = MaxPooling2D((2,2))(x)\n    x = Flatten()(x); x = Dropout(0.5)(x)\n    return Model(inputs, x, name=\"CNN3\")\n\ndef build_cnn4(input_shape):\n    inputs = Input(shape=input_shape)\n    x = Conv2D(32,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(inputs)\n    x = AveragePooling2D((3,3))(x)\n    x = Conv2D(32,(3,3),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = AveragePooling2D((5,5))(x)\n    x = Conv2D(64,(5,5),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = AveragePooling2D((3,3))(x)\n    x = Conv2D(128,(5,5),activation='relu',padding=\"same\",kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = AveragePooling2D((3,3))(x)\n    x = Flatten()(x); x = Dropout(0.5)(x)\n    return Model(inputs, x, name=\"CNN4\")\n\ninput_shape = (224,224,3)\nCNN1, CNN2, CNN3, CNN4 = build_cnn1(input_shape), build_cnn2(input_shape), build_cnn3(input_shape), build_cnn4(input_shape)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# =============================\n# 4. Feature Extraction (with caching)\n# =============================\ndef get_or_compute_features(model, X_images, name):\n    cache_file = os.path.join(cache_dir, f\"{name}.npy\")\n    if os.path.exists(cache_file):\n        print(f\"üîÑ Loading cached {name} features...\")\n        return np.load(cache_file)\n    print(f\"‚ö° Extracting {name} features...\")\n    with tf.device('/GPU:0'):\n        feats = model.predict(X_images, batch_size=128, verbose=1)\n    np.save(cache_file, feats)\n    return feats\n\nfeatures_cnn1 = get_or_compute_features(CNN1, X_images, \"features_cnn1\")\nfeatures_cnn2 = get_or_compute_features(CNN2, X_images, \"features_cnn2\")\nfeatures_cnn3 = get_or_compute_features(CNN3, X_images, \"features_cnn3\")\nfeatures_cnn4 = get_or_compute_features(CNN4, X_images, \"features_cnn4\")\n\n# =============================\n# 5. Dimensionality Reduction (with caching)\n# =============================\nmerged_cache = os.path.join(cache_dir, \"merged_features.npy\")\n\nif os.path.exists(merged_cache):\n    print(\"üîÑ Loading merged PCA+FA features from cache...\")\n    merged_features = np.load(merged_cache)\nelse:\n    print(\"Applying PCA on CNN1+CNN2...\")\n    all_features_dual1 = np.concatenate([features_cnn1, features_cnn2], axis=1)\n    if use_cuml:\n        pca = cuPCA(n_components=50)\n        features_pca = pca.fit_transform(cp.asarray(all_features_dual1))\n        features_pca = cp.asnumpy(features_pca)\n    else:\n        pca = PCA(n_components=50)\n        features_pca = pca.fit_transform(all_features_dual1)\n\n    print(\"Applying FA on CNN3+CNN4...\")\n    all_features_dual2 = np.concatenate([features_cnn3, features_cnn4], axis=1)\n    if use_cuml:\n        fa = cuFA(n_components=50, random_state=42)\n        features_fa = fa.fit_transform(cp.asarray(all_features_dual2))\n        features_fa = cp.asnumpy(features_fa)\n    else:\n        fa = FactorAnalysis(n_components=50, random_state=42)\n        features_fa = fa.fit_transform(all_features_dual2)\n\n    merged_features = np.concatenate([features_pca, features_fa], axis=1)\n    np.save(merged_cache, merged_features)\n\nprint(\"‚úÖ Features ready:\", merged_features.shape)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# =============================\n# 6. Train/Val/Test Split\n# =============================\nX_train, X_temp, y_train, y_temp = train_test_split(\n    merged_features, y, test_size=0.3, random_state=42, stratify=y\n)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=1/3, random_state=42, stratify=y_temp\n)\n\nprint(\"Train:\", X_train.shape, y_train.shape)\nprint(\"Val:  \", X_val.shape, y_val.shape)\nprint(\"Test: \", X_test.shape, y_test.shape)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# =============================\n# 7. Classifier Training\n# =============================\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel_path = os.path.join(base_dir, \"saved_models/dual_cnn_classifier.h5\")\nos.makedirs(os.path.dirname(model_path), exist_ok=True)\n\nif os.path.exists(model_path):\n    print(\"üîÑ Loading saved classifier model...\")\n    from tensorflow.keras.models import load_model\n    model = load_model(model_path)\nelse:\n    print(\"‚ö° Training new classifier...\")\n    model = Sequential([\n        Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n        Dropout(0.2),\n        Dense(128, activation='relu'),\n        Dropout(0.2),\n        Dense(64, activation='relu'),\n        Dropout(0.2),\n        Dense(7, activation='softmax')\n    ])\n\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n    model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=80,\n        batch_size=128,\n        callbacks=[early_stop],\n        verbose=1\n    )\n    model.save(model_path)\n    print(f\"üíæ Model saved at {model_path}\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================\n# 8. Evaluation & Save\n# =============================\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.preprocessing import label_binarize\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntest_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\nprint(f\"‚úÖ Test Accuracy: {test_acc:.4f} | Loss: {test_loss:.4f}\")\n\ny_pred_probs = model.predict(X_test)\ny_pred = np.argmax(y_pred_probs, axis=1)\nprint(\"\\nüìä Classification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=[f\"Class {i}\" for i in range(7)],\n            yticklabels=[f\"Class {i}\" for i in range(7)])\nplt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix\")\nplt.show()\n\ny_test_bin = label_binarize(y_test, classes=np.arange(7))\nroc_auc = roc_auc_score(y_test_bin, y_pred_probs, multi_class=\"ovr\")\nprint(f\" ROC-AUC (macro): {roc_auc:.4f}\")","metadata":{},"outputs":[],"execution_count":null}]}